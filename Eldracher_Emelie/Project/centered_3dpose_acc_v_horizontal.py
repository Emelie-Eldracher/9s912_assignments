# -*- coding: utf-8 -*-
"""centered 3dpose acc/v horizontal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KXuYMAxURXjIGDFy0AHhKfyvm5t61BPb
"""

import tensorflow as tf
import tensorflow_hub as hub

model = hub.load('https://bit.ly/metrabs_s')  # Takes about 3 minutes
! wget -q https://istvansarandi.com/eccv22_demo/test.jpg
img = tf.image.decode_image(tf.io.read_file('test.jpg'))
pred = model.detect_poses(img, skeleton='smpl+head_30')
pred['poses3d'].shape

def plot_results(image, pred, joint_names, joint_edges):
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    from matplotlib.patches import Rectangle
    fig = plt.figure(figsize=(10, 5.2))
    image_ax = fig.add_subplot(1, 2, 1)
    image_ax.imshow(image.numpy())

    # Find the index of the largest person
    largest_person_index = find_largest_person(pred)

    for idx, (x, y, w, h, c) in enumerate(pred['boxes'].numpy()):
        rect = Rectangle((x, y), w, h, fill=False)

        # Set the color of the largest person's bounding box to red, others to blue
        rect.set_edgecolor('r' if idx == largest_person_index else 'b')
        rect.set_linewidth(2)
        image_ax.add_patch(rect)

        # Plot the skeleton of the largest person in green, others in gray
        color = 'g' if idx == largest_person_index else 'gray'

        # Extract the 2D pose of the current person
        pose2d = pred['poses2d'].numpy()[idx]

        for i_start, i_end in joint_edges:
            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=color, marker='o', markersize=2)

    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
    pose_ax.view_init(5, -75)
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(2000, 5000)
    poses3d = pred['poses3d'].numpy()
    print(poses3d)
    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]
    for pose3d in poses3d:
        for i_start, i_end in joint_edges:
            pose_ax.plot(*zip(pose3d[i_start], pose3d[i_end]), marker='o', markersize=2)

    #origin = np.zeros(3)
    #pose_ax.quiver(*origin, 2000, 0, 0, color='r', label='X-axis')
    #pose_ax.quiver(*origin, 0, 2000, 0, color='g', label='Y-axis')
    #pose_ax.quiver(*origin, 0, 0, 2000, color='b', label='Z-axis')

    #plt.legend()
    plt.show()

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Example usage:
joint_names = model.per_skeleton_joint_names['smpl+head_30'].numpy().astype(str)
joint_edges = model.per_skeleton_joint_edges['smpl+head_30'].numpy()
plot_results(img, pred, joint_names, joint_edges)
print(joint_names)
print(joint_edges)

###########2d plot of video working


import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
from matplotlib.patches import Rectangle
import matplotlib.pyplot as plt


# Open the video file
cap = cv2.VideoCapture('/content/mangan.mp4')

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    if pred['boxes'].shape[0] > 0:
      print("yes")

      # Plot results on the current frame
      fig, image_ax = plt.subplots(1, 1, figsize=(10, 5.2))

      image_ax.imshow(frame)

      # Find the index of the largest person
      largest_person_index = find_largest_person(pred)

      for idx, (x, y, w, h, c) in enumerate(pred['boxes'].numpy()):
          rect = Rectangle((x, y), w, h, fill=False)

          # Set the color of the largest person's bounding box to red, others to blue
          rect.set_edgecolor('r' if idx == largest_person_index else 'b')
          rect.set_linewidth(2)
          image_ax.add_patch(rect)

          # Plot the skeleton of the largest person in green, others in gray
          color = 'g' if idx == largest_person_index else 'gray'

          # Extract the 2D pose of the current person
          pose2d = pred['poses2d'].numpy()[idx]

          for i_start, i_end in joint_edges:
              image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=color, marker='o', markersize=2)

      plt.show()

    # Press 'q' to exit the loop and close the video window
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture
cap.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

############ 3d video plot

import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
from matplotlib.patches import Rectangle
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Open the video file
cap = cv2.VideoCapture('/content/mangan.mp4')

counter = 0

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Process each frame of the video
while True:
    counter +=1
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    if pred['boxes'].shape[0] > 0:

      # Find the index of the largest person
      largest_person_index = find_largest_person(pred)

      # Plot results on the current frame
      fig, (image_ax, pose_ax) = plt.subplots(1, 2, figsize=(15, 5.2))

      image_ax.imshow(frame)

      for idx, (x, y, w, h, c) in enumerate(pred['boxes'].numpy()):
          rect = Rectangle((x, y), w, h, fill=False)

          # Set the color of the largest person's bounding box to red, others to blue
          rect.set_edgecolor('r' if idx == largest_person_index else 'b')
          rect.set_linewidth(2)
          image_ax.add_patch(rect)

          # Plot the 2D skeleton of the largest person in green, others in gray
          color = 'g' if idx == largest_person_index else 'gray'
          pose2d = pred['poses2d'].numpy()[idx]
          for i_start, i_end in joint_edges:
              image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=color, marker='o', markersize=2)

      pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
      pose_ax.view_init(5, -75)
      pose_ax.set_xlim3d(-1500, 1500)
      pose_ax.set_zlim3d(-1500, 1500)
      pose_ax.set_ylim3d(2000, 5000)
      poses3d = pred['poses3d'].numpy()
      poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

      for idx, pose3d in enumerate(poses3d):
          # Plot the 3D skeleton of the largest person in a different color
          color = 'g' if idx == largest_person_index else 'gray'
          for i_start, i_end in joint_edges:
              pose_ax.plot(*zip(pose3d[i_start], pose3d[i_end]), color=color, marker='o', markersize=2)

      plt.show()

      print(counter)

      # Press 'q' to exit the loop and close the video window
      if cv2.waitKey(1) & 0xFF == ord('q'):
          break

# Release the video capture
cap.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

#######only 3d ploot biggest person
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
from matplotlib.patches import Rectangle
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Open the video file
cap = cv2.VideoCapture('/content/mangan.mp4')

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')



    # Find the index of the largest person
    largest_person_index = find_largest_person(pred)

    # Plot only the 3D skeleton of the largest person
    fig = plt.figure(figsize=(8, 5))
    pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
    pose_ax.view_init(0, -90)
    pose_ax.set_xlim3d(-500, 1200)
    pose_ax.set_zlim3d(-600,1200)
    pose_ax.set_ylim3d(500, 1500)
    poses3d = pred['poses3d'].numpy()
    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

    # Extract left hip coordinates
    pelvis_coords = poses3d[largest_person_index, 0]
    # Translation to move left hip to the origin
    translation_vector = -pelvis_coords
    translated_keypoints_array = poses3d + translation_vector

    # Plot the 3D skeleton of the largest person in a different color
    color = 'g'
    for i_start, i_end in joint_edges:
        pose_ax.plot(*zip(translated_keypoints_array[largest_person_index, i_start], translated_keypoints_array[largest_person_index, i_end]), color=color, marker='o', markersize=2)

    plt.show()


    # Press 'q' to exit the loop and close the video window
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture
cap.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from mpl_toolkits.mplot3d import Axes3D
import imageio

# Open the video file
cap = cv2.VideoCapture('/content/sarah.mp4')

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Store frames in a list
frames = []

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Find the index of the largest person
    largest_person_index = find_largest_person(pred)

    # Plot results on the current frame
    fig, (image_ax, pose_ax) = plt.subplots(1, 2, figsize=(15, 5.2))

    image_ax.imshow(frame)

    # Set aspect ratio of the image axis
    image_ax.set_aspect('equal')

    # Set consistent axis limits for the image axis
    image_ax.set_xlim(0, frame.shape[1])
    image_ax.set_ylim(frame.shape[0], 0)  # Note the reversed order for y-axis

    for idx, (x, y, w, h, c) in enumerate(pred['boxes'].numpy()):
        rect = Rectangle((x, y), w, h, fill=False)

        # Set the color of the largest person's bounding box to red, others to blue
        rect.set_edgecolor('r' if idx == largest_person_index else 'b')
        rect.set_linewidth(2)
        image_ax.add_patch(rect)

        # Plot the 2D skeleton of the largest person in green, others in gray
        color = 'g' if idx == largest_person_index else 'gray'
        pose2d = pred['poses2d'].numpy()[idx]
        for i_start, i_end in joint_edges:
            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=color, marker='o', markersize=2)

    # Create the 3D subplot separately
    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
    pose_ax.view_init(0, -90)
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(-1500, 5000)

    # Remove the extra 2D axes around the 3D plot
    pose_ax.spines['top'].set_visible(False)
    pose_ax.spines['right'].set_visible(False)
    pose_ax.spines['left'].set_visible(False)
    pose_ax.spines['bottom'].set_visible(False)

    poses3d = pred['poses3d'].numpy()
    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

    # Plot the 3D skeleton of the largest person in a different color
    color = 'g'
    for i_start, i_end in joint_edges:
        pose_ax.plot(*zip(poses3d[largest_person_index, i_start], poses3d[largest_person_index, i_end]), color=color, marker='o', markersize=2)

    # Save the figure to a temporary file
    temp_file_path = 'temppp_3d_skeleton_plot.png'
    fig.savefig(temp_file_path, bbox_inches='tight')

    # Read the saved image and convert it to a frame
    temp_frame = cv2.imread(temp_file_path)
    frames.append(temp_frame)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = '3d_skeletons_2D_3d.gif'
imageio.mimsave(output_gif_path, frames, duration=0.03, fps=30)  # Adjust the duration as needed

#########WORKING GIF, 2d & 3D

import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
from mpl_toolkits.mplot3d import Axes3D
import imageio

# Open the video file
cap = cv2.VideoCapture('/content/sarah.mp4')

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Store frames in a list
frames = []

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Find the index of the largest person
    largest_person_index = find_largest_person(pred)

    # Plot results on the current frame
    fig, (image_ax, pose_ax) = plt.subplots(1, 2, figsize=(15, 5.2))

    image_ax.imshow(frame)

    # Set aspect ratio of the image axis
    image_ax.set_aspect('equal')

    # Set consistent axis limits for the image axis
    image_ax.set_xlim(0, frame.shape[1])
    image_ax.set_ylim(frame.shape[0], 0)  # Note the reversed order for y-axis

    for idx, (x, y, w, h, c) in enumerate(pred['boxes'].numpy()):
        rect = Rectangle((x, y), w, h, fill=False)

        # Set the color of the largest person's bounding box to red, others to blue
        rect.set_edgecolor('r' if idx == largest_person_index else 'b')
        rect.set_linewidth(2)
        image_ax.add_patch(rect)

        # Plot the 2D skeleton of the largest person in green, others in gray
        color = 'g' if idx == largest_person_index else 'gray'
        pose2d = pred['poses2d'].numpy()[idx]
        for i_start, i_end in joint_edges:
            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), color=color, marker='o', markersize=2)

    # Plot only the 3D skeleton of the largest person
    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
    pose_ax.view_init(0, -90)
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(-1500, 5000)

    poses3d = pred['poses3d'].numpy()
    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

    # Plot the 3D skeleton of the largest person in a different color
    color = 'g'
    for i_start, i_end in joint_edges:
        pose_ax.plot(*zip(poses3d[largest_person_index, i_start], poses3d[largest_person_index, i_end]), color=color, marker='o', markersize=2)

    # Save the figure to a temporary file
    temp_file_path = 'temppp_3d_skeleton_plot.png'
    fig.savefig(temp_file_path, bbox_inches='tight')
    plt.close(fig)

    # Read the saved image and convert it to a frame
    temp_frame = cv2.imread(temp_file_path)
    frames.append(temp_frame)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = '3d_skeletons_2D_3d_final.gif'
imageio.mimsave(output_gif_path, frames, duration=0.03, fps=30)  # Adjust the duration as needed

#########WORKING GIDf WOOHOOO
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import imageio

# Open the video file
cap = cv2.VideoCapture('/content/ben.mp4')

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Store frames in a list
frames = []

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Find the index of the largest person
    largest_person_index = find_largest_person(pred)

    # Plot only the 3D skeleton of the largest person
    fig = plt.figure(figsize=(8, 5))
    pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
    pose_ax.view_init(5, -75)
    pose_ax.set_xlim3d(-1500, 1500)
    pose_ax.set_zlim3d(-1500, 1500)
    pose_ax.set_ylim3d(-1500, 5000)
    poses3d = pred['poses3d'].numpy()
    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

    # Plot the 3D skeleton of the largest person in a different color
    color = 'g'
    for i_start, i_end in joint_edges:
        pose_ax.plot(*zip(poses3d[largest_person_index, i_start], poses3d[largest_person_index, i_end]), color=color, marker='o', markersize=2)

    # Save the figure to a temporary file
    temp_file_path = 'temppp_3d_skeleton_plot.png'
    fig.savefig(temp_file_path, bbox_inches='tight')
    plt.close(fig)

    # Read the saved image and convert it to a frame
    temp_frame = cv2.imread(temp_file_path)
    frames.append(temp_frame)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = '3d_skeletons.gif'
imageio.mimsave(output_gif_path, frames, duration=0.1)  # Adjust the duration as needed

######GET CSV with gif NOT CENTERED AROUND ORIGIN
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import imageio
import pandas as pd
import csv

# Open the video file
cap = cv2.VideoCapture('/content/ben.mp4')

# Create a DataFrame to store keypoints
columns = ['Frame']
for i in range(len(joint_names)):  # Assuming there are 30 keypoints
    #columns.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
    columns.extend([joint_names[i]+"_x", joint_names[i]+"_y", joint_names[i]+"_z"])
keypoints_df = pd.DataFrame(columns=columns)

# Create a list to store new_list_keypoints for writing to CSV
new_list_keypoints = ["Frame"]
#for i in range(1, 31):  # Assuming there are 30 keypoints
    #new_list_keypoints.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

# Replace 'output_file.csv' with the desired output CSV file name
output_file = 'output_file.csv'

# Write the header row to the CSV file
with open(output_file, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Store frames in a list
frames = []

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Extract and save keypoints to DataFrame
    keypoints = [int(cap.get(1))]
    if pred['boxes'].shape[0] > 0:
      for keypoint in np.array(pred['poses3d'][0]).flatten():
          keypoints.append(keypoint/1000)
      keypoints_df = pd.concat([keypoints_df, pd.DataFrame([keypoints], columns=columns)], ignore_index=True)

      # Find the index of the largest person
      largest_person_index = find_largest_person(pred)

      # Plot only the 3D skeleton of the largest person
      fig = plt.figure(figsize=(8, 5))
      pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
      pose_ax.view_init(0, -90)
      pose_ax.set_xlim3d(-1000, 1200)
      pose_ax.set_zlim3d(-1000,1200)
      pose_ax.set_ylim3d(0, 1500)
      poses3d = pred['poses3d'].numpy()
      poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

        # Extract pelvis coordinates
      pelvis_coords = poses3d[largest_person_index, 0]
      # Translation to move left hip to the origin
      translation_vector = -pelvis_coords
      translated_keypoints_array = poses3d + translation_vector

      print(translated_keypoints_array.shape)

      # Plot the 3D skeleton of the largest person in a different color
      color = 'g'
      for i_start, i_end in joint_edges:
          pose_ax.plot(*zip(translated_keypoints_array[largest_person_index, i_start], translated_keypoints_array[largest_person_index, i_end]), color=color, marker='o', markersize=2)

      # Save the figure to a temporary file
      temp_file_path = 'temppp_3d_skeleton_plot.png'
      fig.savefig(temp_file_path, bbox_inches='tight')
      plt.close(fig)

      # Read the saved image and convert it to a frame
      temp_frame = cv2.imread(temp_file_path)
      frames.append(temp_frame)

# Save keypoints DataFrame to a CSV file
keypoints_df.to_csv('keypoints_data.csv', index=False)

# Append new_list_keypoints to the CSV file for the header row
with open(output_file, mode='a', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Append keypoints to the CSV file
keypoints_df.to_csv(output_file, mode='a', header=False, index=False)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = '3d_skeletons.gif'
imageio.mimsave(output_gif_path, frames, duration=0.1)  # Adjust the duration as needed

######GET CSV with gif ORIGIN
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import imageio
import pandas as pd
import csv

# Open the video file
cap = cv2.VideoCapture('/content/IMG_4772.MOV')

# Create a DataFrame to store keypoints
columns = ['Frame']
for i in range(len(joint_names)):  # Assuming there are 30 keypoints
    #columns.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
    columns.extend([joint_names[i]+"_x", joint_names[i]+"_y", joint_names[i]+"_z"])
keypoints_df = pd.DataFrame(columns=columns)

# Create a list to store new_list_keypoints for writing to CSV
new_list_keypoints = ["Frame"]
#for i in range(1, 31):  # Assuming there are 30 keypoints
    #new_list_keypoints.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

# Replace 'output_file.csv' with the desired output CSV file name
output_file = 'output_file.csv'

# Write the header row to the CSV file
with open(output_file, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Store frames in a list
frames = []

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Extract and save keypoints to DataFrame
    keypoints = [int(cap.get(1))]
    if pred['boxes'].shape[0] > 0:
      #for keypoint in np.array(pred['poses3d'][0]).flatten():
          #keypoints.append(keypoint/1000)
      #keypoints_df = pd.concat([keypoints_df, pd.DataFrame([keypoints], columns=columns)], ignore_index=True)

      # Find the index of the largest person
      largest_person_index = find_largest_person(pred)

      # Plot only the 3D skeleton of the largest person
      fig = plt.figure(figsize=(8, 5))
      pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
      pose_ax.view_init(0, -90)
      pose_ax.set_xlim3d(-1000, 1200)
      pose_ax.set_zlim3d(-1000,1200)
      pose_ax.set_ylim3d(0, 1500)
      poses3d = pred['poses3d'].numpy()
      #poses3d[..., 0] =  poses3d[..., 0] *-1
      poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

      # Extract pelvis coordinates
      pelvis_coords = poses3d[largest_person_index, 0]
      # Translation to move left hip to the origin
      translation_vector = -pelvis_coords
      translated_keypoints_array = poses3d + translation_vector
      translated_keypoints_array[..., 0] = -translated_keypoints_array[..., 0]

      for keypoint in np.array(translated_keypoints_array[largest_person_index]).flatten():
          keypoints.append(keypoint/1000)
      keypoints_df = pd.concat([keypoints_df, pd.DataFrame([keypoints], columns=columns)], ignore_index=True)

      # Plot the 3D skeleton of the largest person in a different color
      color = 'g'
      for i_start, i_end in joint_edges:
          pose_ax.plot(*zip(translated_keypoints_array[largest_person_index, i_start], translated_keypoints_array[largest_person_index, i_end]), color=color, marker='o', markersize=2)

      # Save the figure to a temporary file
      temp_file_path = 'temppp_3d_skeleton_plot.png'
      fig.savefig(temp_file_path, bbox_inches='tight')
      plt.close(fig)

      # Read the saved image and convert it to a frame
      temp_frame = cv2.imread(temp_file_path)
      frames.append(temp_frame)

# Save keypoints DataFrame to a CSV file
keypoints_df.to_csv('keypoints_data_centered_PR1_2.csv', index=False)

# Append new_list_keypoints to the CSV file for the header row
with open(output_file, mode='a', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Append keypoints to the CSV file
keypoints_df.to_csv(output_file, mode='a', header=False, index=False)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = 'pr1_2.gif'
imageio.mimsave(output_gif_path, frames, duration=0.03, fps = 30)  # Adjust the duration as needed
print("done")

######GET CSV with gifCENTERRRRR??????
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import imageio
import pandas as pd
import csv

# Open the video file
cap = cv2.VideoCapture('/content/mangan.mp4')

# Create a DataFrame to store keypoints
columns = ['Frame']
for i in range(len(joint_names)):  # Assuming there are 30 keypoints
    #columns.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
    columns.extend([joint_names[i]+"_x", joint_names[i]+"_y", joint_names[i]+"_z"])
keypoints_df = pd.DataFrame(columns=columns)

# Create a list to store new_list_keypoints for writing to CSV
new_list_keypoints = ["Frame"]
#for i in range(1, 31):  # Assuming there are 30 keypoints
    #new_list_keypoints.extend([f'Keypoint_{i}_x', f'Keypoint_{i}_y', f'Keypoint_{i}_z'])
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

# Replace 'output_file.csv' with the desired output CSV file name
output_file = 'output_file.csv'

# Write the header row to the CSV file
with open(output_file, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Store frames in a list
frames = []

def find_largest_person(pred):
    # Assuming pred is a dictionary with a 'boxes' key containing bounding box coordinates
    boxes = pred['boxes'].numpy()

    # Calculate the area of each bounding box
    areas = boxes[:, 2] * boxes[:, 3]

    # Find the index of the bounding box with the largest area
    index_of_largest_person = areas.argmax()

    return index_of_largest_person

def rotate_skeleton_for_horizontal_view(poses3d):

  pelvis_index = 0
  # Calculate the rotation angle based on the head's x and y coordinates
  angle = np.arctan2(poses3d[pelvis_index, 1], poses3d[pelvis_index, 0])+3.14159
  print(angle)
  # Apply the rotation transformation to all points
  rotated_poses3d = np.zeros_like(poses3d)
  #rotated_poses3d[..., 0] = np.sin(-(6.282-angle)) * poses3d[..., 1]  # Swap x and y coordinates and flip horizontally
  #rotated_poses3d[..., 1] = np.cos(-(6.282-angle)) * poses3d[..., 1]   # Set y-coordinate to a constant value
  #rotated_poses3d[..., 2] = poses3d[..., 2]

  head_index = 0

  # Translation to move head coordinates to the origin
  translation_vector = -poses3d[..., head_index, :]

  # Apply translation to all points
  translated_poses3d = poses3d + np.expand_dims(translation_vector, axis=-2)

  # Calculate the rotation angle based on the translated head's x and y coordinates
  angle = np.arctan2(translated_poses3d[..., head_index, 1], translated_poses3d[..., head_index, 0])

  # Apply the rotation transformation to all points
  rotated_poses3d = np.zeros_like(translated_poses3d)
  rotated_poses3d[..., 0] = np.cos(angle) * translated_poses3d[..., 0] - np.sin(angle) * translated_poses3d[..., 1]
  rotated_poses3d[..., 1] = np.sin(angle) * translated_poses3d[..., 0] + np.cos(angle) * translated_poses3d[..., 1]
  rotated_poses3d[..., 2] = translated_poses3d[..., 2]

  return rotated_poses3d



  #####new
 # pelvis_coords = poses3d[0]
  #Translation to move left hip to the origin
 # translation_vector = -pelvis_coords
  #translated_frame_poses = poses3d + translation_vector

  return rotated_poses3d

# Process each frame of the video
while True:
    ret, frame = cap.read()

    if not ret:
        break  # Break the loop if we reach the end of the video

    # Preprocess the frame to be in the same format as the input image
    img = tf.image.decode_image(tf.image.encode_jpeg(tf.convert_to_tensor(frame)))

    # Make a prediction for the current frame
    pred = model.detect_poses(img, skeleton='smpl+head_30')

    # Extract and save keypoints to DataFrame
    keypoints = [int(cap.get(1))]
    if pred['boxes'].shape[0] > 0:
      #for keypoint in np.array(pred['poses3d'][0]).flatten():
          #keypoints.append(keypoint/1000)
      #keypoints_df = pd.concat([keypoints_df, pd.DataFrame([keypoints], columns=columns)], ignore_index=True)

      # Find the index of the largest person
      largest_person_index = find_largest_person(pred)

      # Plot only the 3D skeleton of the largest person
      fig = plt.figure(figsize=(8, 5))
      pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
      pose_ax.view_init(0, -90)
      pose_ax.set_xlim3d(-1000, 1200)
      pose_ax.set_zlim3d(-1000,1200)
      pose_ax.set_ylim3d(0, 1500)
      poses3d = pred['poses3d'].numpy()
      #poses3d[..., 0] =  poses3d[..., 0] *-1
      poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]

      print(poses3d[largest_person_index].shape)

      rotate = rotate_skeleton_for_horizontal_view(poses3d[largest_person_index])
      # Extract pelvis coordinates
      pelvis_coords = rotate[0]
      # Translation to move left hip to the origin
      translation_vector = -pelvis_coords
      translated_keypoints_array = rotate + translation_vector
      #translated_keypoints_array[..., 0] = -translated_keypoints_array[..., 0]

      for keypoint in np.array(translated_keypoints_array).flatten():
          keypoints.append(keypoint/1000)
      keypoints_df = pd.concat([keypoints_df, pd.DataFrame([keypoints], columns=columns)], ignore_index=True)

      # Plot the 3D skeleton of the largest person in a different color
      color = 'g'
      for i_start, i_end in joint_edges:
          pose_ax.plot(*zip(translated_keypoints_array[i_start], translated_keypoints_array[i_end]), color=color, marker='o', markersize=2)

      # Save the figure to a temporary file
      temp_file_path = 'temppp_3d_skeleton_plot.png'
      fig.savefig(temp_file_path, bbox_inches='tight')
      plt.close(fig)

      # Read the saved image and convert it to a frame
      temp_frame = cv2.imread(temp_file_path)
      frames.append(temp_frame)

# Save keypoints DataFrame to a CSV file
keypoints_df.to_csv('keypoints_test.csv', index=False)

# Append new_list_keypoints to the CSV file for the header row
with open(output_file, mode='a', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(new_list_keypoints)

# Append keypoints to the CSV file
keypoints_df.to_csv(output_file, mode='a', header=False, index=False)

# Release the video capture
cap.release()

# Save frames as a GIF
output_gif_path = 'TEST.gif'
imageio.mimsave(output_gif_path, frames, duration=0.03, fps = 30)  # Adjust the duration as needed
print("done")

####GETS NEW WITH VELOCITY AND ACC AND ANGLES
import pandas as pd
import numpy as np

# Load the CSV data
data = pd.read_csv('keypoints_data_centered_PR1_2.csv')

# Assuming 'Frame' is the column representing the frame number
# 'X_1', 'Y_1', 'Z_1', ..., 'X_n', 'Y_n', 'Z_n' are the columns representing keypoints
new_list_keypoints = ["Frame"]
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

keypoint_columns = new_list_keypoints

# Calculate time in seconds
fps = 30  # Replace with your actual frames per second
data['Time'] = data['Frame'] / fps

# Calculate velocity (change in position per time)
for column in keypoint_columns:
    data[f'V_{column}'] = np.gradient(data[column], data['Time'])

# Calculate acceleration (change in velocity per time)
for column in keypoint_columns:
    data[f'A_{column}'] = np.gradient(data[f'V_{column}'], data['Time'])

# Calculate angles between joints based on joint_edges
for edge in joint_edges:
    i, j = edge
    x_col1, y_col1, z_col1 = f'{joint_names[i]}_x', f'{joint_names[i]}_y', f'{joint_names[i]}_z'
    x_col2, y_col2, z_col2 = f'{joint_names[j]}_x', f'{joint_names[j]}_y', f'{joint_names[j]}_z'

    # Calculate vectors between joints
    vector1 = data[[x_col2, y_col2, z_col2]].values - data[[x_col1, y_col1, z_col1]].values
    vector2 = data[[f'{joint_names[j]}_x', f'{joint_names[j]}_y', f'{joint_names[j]}_z']].values - data[[x_col2, y_col2, z_col2]].values

    # Calculate dot product and magnitudes
    dot_product = np.sum(vector1 * vector2, axis=1)
    magnitude1 = np.linalg.norm(vector1, axis=1)
    magnitude2 = np.linalg.norm(vector2, axis=1)

    # Calculate angle (in radians) using the dot product formula
    angle_column_name = f'Angle_{joint_names[i]}_{joint_names[j]}'
    data[angle_column_name] = np.arccos(dot_product / (magnitude1 * magnitude2))
    print("dne")

# Save the DataFrame to a new CSV file
output_csv_path = 'BEN_____PR2_1_2_coordinates_v_a_angle.csv'
data.to_csv(output_csv_path, index=False)

####GETS NEW WITH VELOCITY AND ACC AND SPECIFIC ANGLES
import pandas as pd
import numpy as np

# Load the CSV data
data = pd.read_csv('keypoints_data_centered_PR1_2.csv')

# Assuming 'Frame' is the column representing the frame number
# 'X_1', 'Y_1', 'Z_1', ..., 'X_n', 'Y_n', 'Z_n' are the columns representing keypoints
new_list_keypoints = ["Frame"]
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

keypoint_columns = new_list_keypoints

# Calculate time in seconds
fps = 30  # Replace with your actual frames per second
data['Time'] = data['Frame'] / fps

# Calculate velocity (change in position per time)
for column in keypoint_columns:
    data[f'V_{column}'] = np.gradient(data[column], data['Time'])

# Calculate acceleration (change in velocity per time)
for column in keypoint_columns:
    data[f'A_{column}'] = np.gradient(data[f'V_{column}'], data['Time'])

for edge in joint_edges:
  print(edge)

for i in joint_names:
  print(i)

# Calculate angles between joints based on joint_edges
for edge in joint_edges:
    i, j = edge
    x_col1, y_col1, z_col1 = f'{joint_names[i]}_x', f'{joint_names[i]}_y', f'{joint_names[i]}_z'
    x_col2, y_col2, z_col2 = f'{joint_names[j]}_x', f'{joint_names[j]}_y', f'{joint_names[j]}_z'

    # Calculate vectors between joints
    vector1 = data[[x_col2, y_col2, z_col2]].values - data[[x_col1, y_col1, z_col1]].values
    vector2 = data[[f'{joint_names[j]}_x', f'{joint_names[j]}_y', f'{joint_names[j]}_z']].values - data[[x_col2, y_col2, z_col2]].values

    # Calculate dot product and magnitudes
    dot_product = np.sum(vector1 * vector2, axis=1)
    magnitude1 = np.linalg.norm(vector1, axis=1)
    magnitude2 = np.linalg.norm(vector2, axis=1)

    # Calculate angle (in radians) using the dot product formula
    angle_column_name = f'Angle_{joint_names[i]}_{joint_names[j]}'
    data[angle_column_name] = np.arccos(dot_product / (magnitude1 * magnitude2))
    print("dne")

# Save the DataFrame to a new CSV file
output_csv_path = 'PR2_1_2_coordinates_v_a_angle.csv'
data.to_csv(output_csv_path, index=False)

####GETS NEW WITH VELOCITY AND ACC
import pandas as pd
import numpy as np

# Load the CSV data
data = pd.read_csv('keypoints_data_centered_PR1.csv'.csv')

# Assuming 'Frame' is the column representing the frame number
# 'X_1', 'Y_1', 'Z_1', ..., 'X_n', 'Y_n', 'Z_n' are the columns representing keypoints
#keypoint_columns = [f'X_{i}', f'Y_{i}', f'Z_{i}' for i in range(1, 31)]

new_list_keypoints = ["Frame"]
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

keypoint_columns = new_list_keypoints

# Calculate time in seconds
fps = 30  # Replace with your actual frames per second
data['Time'] = data['Frame'] / fps

# Calculate velocity (change in position per time)
for column in keypoint_columns:
    data[f'V_{column}'] = np.gradient(data[column], data['Time'])

# Calculate acceleration (change in velocity per time)
for column in keypoint_columns:
    data[f'A_{column}'] = np.gradient(data[f'V_{column}'], data['Time'])

# Save the DataFrame to a new CSV file
output_csv_path = 'keypoints_with_velocity_acceleration.csv'
data.to_csv(output_csv_path, index=False)

print(f"The DataFrame with velocity and acceleration has been saved to {output_csv_path}.")



#####PLOT FRAME BIGGEST VELOCITY

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the CSV data
data = pd.read_csv('keypoints_with_velocity_acceleration.csv')

# Assuming 'Frame' is the column representing the frame number
# 'X_1', 'Y_1', 'Z_1', ..., 'X_n', 'Y_n', 'Z_n' are the columns representing keypoints
new_list_keypoints = []
for i in range(len(joint_names)):
    new_list_keypoints.append(joint_names[i]+"_x")
    new_list_keypoints.append(joint_names[i]+"_y")
    new_list_keypoints.append(joint_names[i]+"_z")

keypoint_columns = new_list_keypoints
data[keypoint_columns] *= 1000

# Find the keypoint with the highest velocity
keypoint_with_highest_velocity = 'V_lwri_smpl_x'  # Replace with the desired keypoint

# Find the frame with the highest velocity
frame_with_highest_velocity = data.loc[data[keypoint_with_highest_velocity].idxmax(), 'Frame']
frame_with_lowest_velocity = data.loc[data[keypoint_with_highest_velocity].idxmin(), 'Frame']

# Get the spatial coordinates for the frame with the highest velocity
keypoint_data = data[data['Frame'] == frame_with_highest_velocity][keypoint_columns]
keypoint_data = data[data['Frame'] == frame_with_lowest_velocity][keypoint_columns]

# Plot the skeleton for the frame with the highest velocity
fig = plt.figure(figsize=(8, 5))

# Adjust the view_init function to re-orient the figure by 90 degrees

pose_ax = fig.add_subplot(1, 1, 1, projection='3d')
pose_ax.set_xlim3d(-4000, 1500)
pose_ax.set_zlim3d(0, 3000)
pose_ax.set_ylim3d(-1000, 3000)


# ... (your existing code)

# Plot the skeleton with lines connecting keypoints
for i_start, i_end in joint_edges:
    x_start, y_start, z_start = keypoint_data.iloc[:, i_start*3:i_start*3+3].values.T
    x_end, y_end, z_end = keypoint_data.iloc[:, i_end*3:i_end*3+3].values.T

    # Negate y-coordinates to flip the figure upward
    y_start = -y_start
    y_end = -y_end

    # Connect keypoints with lines
    pose_ax.plot([x_start, x_end], [y_start, y_end], [z_start, z_end], color='b', linewidth=2)

# Plot the keypoints
pose_ax.scatter(keypoint_data.iloc[:, ::3].values.flatten() - keypoint_data.iloc[:, ::3].mean(),
                -(keypoint_data.iloc[:, 1::3].values.flatten() - keypoint_data.iloc[:, 1::3].mean()),  # Negate y-coordinates
                keypoint_data.iloc[:, 2::3].values.flatten() - keypoint_data.iloc[:, 2::3].mean(), color='b', marker='o', s=50)

# Add axis lines
pose_ax.quiver(0, 0, 0, 2000, 0, 0, color='r', label='X-axis')
pose_ax.quiver(0, 0, 0, 0, 2000, 0, color='g', label='Y-axis')
pose_ax.quiver(0, 0, 0, 0, 0, 2000, color='b', label='Z-axis')

# Set a horizontal viewing angle and flip the figure upward
pose_ax.view_init(azim=0, elev=90)  # Adjust elev to control the upward flip

# Increase the figure size
fig.set_size_inches(12, 8)

pose_ax.view_init(azim=90, elev=90)

plt.legend()
plt.show()

import plotly.graph_objects as go
import pandas as pd

# Assuming 'data' is a pandas DataFrame with columns 'Time', 'V_rwri_smpl_x', and 'V_rkne_smpl_x'

# Define a window size for the moving average
window_size = 10

# Apply a simple moving average to smooth the curves
data['Smoothed_V_rwri'] = data['A_rwri_smpl_x'].rolling(window=window_size).mean()
data['Smoothed_V_rkne'] = data['A_rkne_smpl_x'].rolling(window=window_size).mean()

# Create traces for original and smoothed curves
#trace_original_rwri = go.Scatter(x=data['Time'], y=data['V_rwri_smpl_x'], mode='lines', name='Original rhand Velocity', line=dict(color='blue'))
trace_smoothed_rwri = go.Scatter(x=data['Time'], y=data['Smoothed_V_rwri'], mode='lines', name='Smoothed rhand Velocity', line=dict(color='orange'))

#trace_original_rkne = go.Scatter(x=data['Time'], y=data['V_rkne_smpl_x'], mode='lines', name='Original rknee Velocity', line=dict(color='green'))
trace_smoothed_rkne = go.Scatter(x=data['Time'], y=data['Smoothed_V_rkne'], mode='lines', name='Smoothed rknee Velocity', line=dict(color='red'))

# Create layout
layout = go.Layout(title='Smoothed Velocity Curves', xaxis=dict(title='Time'), yaxis=dict(title='Velocity'))

# Create figure and add traces
fig = go.Figure(data=[#trace_original_rwri,
                      trace_smoothed_rwri,
                      #trace_original_rkne,
                      trace_smoothed_rkne], layout=layout)

# Show the plot
fig.show()

import pandas as pd
import plotly.graph_objects as go
from scipy.signal import savgol_filter

data = pd.read_csv('/content/PR2_1_2_coordinates_v_a_angle.csv')

# Assuming 'data' is a pandas DataFrame with columns 'Time', 'V_rwri_smpl_x', and 'V_rkne_smpl_x'

# Set the window size and polynomial order for the Savitzky-Golay filter
window_size = 26 #33
poly_order = 3

# Apply Savitzky-Golay filter to smooth the data
smoothed_rwri = -savgol_filter(((data['V_rwri_smpl_x']+data['V_rwri_smpl_x'])/2), window_size, poly_order)
smoothed_rkne = -savgol_filter(data['V_rkne_smpl_x'], window_size, poly_order)
smoothed_rsho = -savgol_filter(data['V_rsho_smpl_x'], window_size, poly_order)
smoothed_relb = -savgol_filter(data['V_relb_smpl_x'], window_size, poly_order)

# Create traces for original and smoothed curves
trace_original_rwri = go.Scatter(x=data['Time'], y=(data['A_rwri_smpl_x']+data['A_lwri_smpl_x'])/2, mode='lines', name='Original rhand Velocity', line=dict(color='blue'))
trace_smoothed_rwri = go.Scatter(x=data['Time'], y=smoothed_rwri, mode='lines', name='Smoothed hand Velocity', line=dict(color='blue'))

trace_original_rkne = go.Scatter(x=data['Time'], y=data['A_rkne_smpl_x'], mode='lines', name='Original rknee Velocity', line=dict(color='green'))
trace_smoothed_rkne = go.Scatter(x=data['Time'], y=smoothed_rkne, mode='lines', name='Smoothed rknee Velocity', line=dict(color='red'))

trace_original_rsho = go.Scatter(x=data['Time'], y=data['A_rsho_smpl_x'], mode='lines', name='Original rsho Velocity', line=dict(color='green'))
trace_smoothed_rsho = go.Scatter(x=data['Time'], y=smoothed_rsho, mode='lines', name='Smoothed rsho Velocity', line=dict(color='green'))

trace_smoothed_relb = go.Scatter(x=data['Time'], y=smoothed_relb, mode='lines', name='Smoothed rebl Velocity', line=dict(color='orange'))

# Create layout
layout = go.Layout(title='Original and Smoothed Velocity Curves', xaxis=dict(title='Time'), yaxis=dict(title='Velocity'))

# Create figure and add traces
fig = go.Figure(data=[#trace_original_rwri,
                      trace_smoothed_rwri,
                     # trace_original_rkne,
                      trace_smoothed_rkne,
                      trace_smoothed_rsho,
                      trace_smoothed_relb,
                      ], layout=layout)

# Show the plot
fig.show()

import pandas as pd
import plotly.graph_objects as go
from scipy.signal import savgol_filter
from scipy.integrate import quad
import matplotlib.pyplot as plt

data = pd.read_csv('/content/PR2_1_2_coordinates_v_a_angle.csv')

# Assuming 'data' is a pandas DataFrame with columns 'Time', 'V_rwri_smpl_x', and 'V_rkne_smpl_x'

# Set the window size and polynomial order for the Savitzky-Golay filter
window_size = 44 #33
poly_order = 3

# Apply Savitzky-Golay filter to smooth the data
smoothed_rwri = -120*savgol_filter(((data['A_rwri_smpl_x']+data['V_rwri_smpl_x'])/2), window_size, poly_order)
smoothed_rkne = -120*savgol_filter(data['A_rkne_smpl_x'], window_size, poly_order)
smoothed_rsho = -120*savgol_filter(data['A_rsho_smpl_x'], window_size, poly_order)
smoothed_relb = -120*savgol_filter(data['A_relb_smpl_x'], window_size, poly_order)

# Create traces for original and smoothed curves
trace_original_rwri = go.Scatter(x=data['Time'], y=(data['A_rwri_smpl_x']+data['A_lwri_smpl_x'])/2, mode='lines', name='Original rhand Velocity', line=dict(color='blue'))
trace_smoothed_rwri = go.Scatter(x=data['Time'], y=smoothed_rwri, mode='lines', name='Smoothed hand Acceleration', line=dict(color='blue'))

trace_original_rkne = go.Scatter(x=data['Time'], y=data['A_rkne_smpl_x'], mode='lines', name='Original rknee Velocity', line=dict(color='green'))
trace_smoothed_rkne = go.Scatter(x=data['Time'], y=smoothed_rkne, mode='lines', name='Smoothed rknee Acceleration', line=dict(color='red'))

trace_original_rsho = go.Scatter(x=data['Time'], y=data['A_rsho_smpl_x'], mode='lines', name='Original rsho Velocity', line=dict(color='green'))
trace_smoothed_rsho = go.Scatter(x=data['Time'], y=smoothed_rsho, mode='lines', name='Smoothed rsho Acceleration', line=dict(color='green'))

trace_smoothed_relb = go.Scatter(x=data['Time'], y=smoothed_relb, mode='lines', name='Smoothed rebl Acceleration', line=dict(color='orange'))

# Create layout
layout = go.Layout(title='Original and Smoothed Acceleration Curves', xaxis=dict(title='Time'), yaxis=dict(title='Acceleration'))

# Create figure and add traces
fig = go.Figure(data=[#trace_original_rwri,
                      trace_smoothed_rwri,
                     # trace_original_rkne,
                      #trace_smoothed_rkne,
                      #trace_smoothed_rsho,
                      #trace_smoothed_relb,
                      ], layout=layout)

# Show the plot
fig.show()

area = np.trapz(smoothed_rwri[33:55], data['Time'][33:55])
print(f"Area under the curve: {area}")

#x = np.array([5,7,8,7,2,17,2,9,4,11,12,9,6])
y = data2['vx'][:63]

print(y)
#plt.scatter(x, y)
#plt.show()

import pandas as pd

# ... (your existing code)
# Load the CSV data
data = pd.read_csv('keypoints_with_velocity_acceleration.csv')

# Plot the velocity over time
plt.figure(figsize=(10, 6))
#plt.plot(data['Time'], data['V_lwri_smpl_x'], label='lhand Velocity')
#plt.plot(data['Time'], data['V_rwri_smpl_x'], label='rhand Velocity')
plt.plot(data['Time'], data['lkne_smpl_x'], label='lknee placex', color = "g")
plt.plot(data['Time'], data['ltoe_smpl_x'], label='ltoe placex', color = "g")
plt.plot(data['Time'], data['lank_smpl_x'], label='lankle placex', color = "g")
plt.plot(data['Time'], data['lkne_smpl_y'], label='lknee placey')
plt.plot(data['Time'], data['ltoe_smpl_y'], label='ltoe placey')
plt.plot(data['Time'], data['lank_smpl_y'], label='lankle placey', color = "r")
#plt.plot(data['Time'], data['V_lsho_smpl_x'], label='lshoulder Velocity')
#plt.plot(data['Time'], data['A_lwri_smpl_x'], label='lhand ACC')
#plt.plot(data['Time'], data['A_rwri_smpl_x'], label='rhand ACC')


plt.xlabel('Time (seconds)')
plt.ylabel('position')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Load the CSV data
data = pd.read_csv('keypoints_with_velocity_acceleration.csv')

keypoint_columns = new_list_keypoints
data[keypoint_columns] *= 1000

# Choose the keypoint representing the hand velocity
hand_velocity_keypoint = 'V_lwri_smpl_x'  # Replace with the actual keypoint name

# Filter data where hand velocity is approximately 0
threshold = 0.1  # Adjust the threshold as needed
filtered_data = data[data[hand_velocity_keypoint].abs() < threshold]

print(filtered_data)
print(keypoints_start)

data = pd.read_csv('/content/keypoints_data.csv')
print(list(data.columns.values))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load your CSV data
data = pd.read_csv('/content/keypoints_data.csv')

# Choose relevant columns for the input data (X) and output data (y)
input_columns = ['pelv_smpl_x', 'pelv_smpl_y', 'pelv_smpl_z', 'lhip_smpl_x', 'lhip_smpl_y', 'lhip_smpl_z', 'rhip_smpl_x', 'rhip_smpl_y', 'rhip_smpl_z', 'bell_smpl_x', 'bell_smpl_y', 'bell_smpl_z', 'lkne_smpl_x', 'lkne_smpl_y', 'lkne_smpl_z', 'rkne_smpl_x', 'rkne_smpl_y', 'rkne_smpl_z', 'spin_smpl_x', 'spin_smpl_y', 'spin_smpl_z', 'lank_smpl_x', 'lank_smpl_y', 'lank_smpl_z', 'rank_smpl_x', 'rank_smpl_y', 'rank_smpl_z', 'thor_smpl_x', 'thor_smpl_y', 'thor_smpl_z', 'ltoe_smpl_x', 'ltoe_smpl_y', 'ltoe_smpl_z', 'rtoe_smpl_x', 'rtoe_smpl_y', 'rtoe_smpl_z', 'neck_smpl_x', 'neck_smpl_y', 'neck_smpl_z', 'lcla_smpl_x', 'lcla_smpl_y', 'lcla_smpl_z', 'rcla_smpl_x', 'rcla_smpl_y', 'rcla_smpl_z', 'head_smpl_x', 'head_smpl_y', 'head_smpl_z', 'lsho_smpl_x', 'lsho_smpl_y', 'lsho_smpl_z', 'rsho_smpl_x', 'rsho_smpl_y', 'rsho_smpl_z', 'lelb_smpl_x', 'lelb_smpl_y', 'lelb_smpl_z', 'relb_smpl_x', 'relb_smpl_y', 'relb_smpl_z', 'lwri_smpl_x', 'lwri_smpl_y', 'lwri_smpl_z', 'rwri_smpl_x', 'rwri_smpl_y', 'rwri_smpl_z', 'lhan_smpl_x', 'lhan_smpl_y', 'lhan_smpl_z', 'rhan_smpl_x', 'rhan_smpl_y', 'rhan_smpl_z', 'nose_coco_x', 'nose_coco_y', 'nose_coco_z', 'leye_coco_x', 'leye_coco_y', 'leye_coco_z', 'lear_coco_x', 'lear_coco_y', 'lear_coco_z', 'reye_coco_x', 'reye_coco_y', 'reye_coco_z', 'rear_coco_x', 'rear_coco_y', 'rear_coco_z', 'htop_mpi_inf_3dhp_x', 'htop_mpi_inf_3dhp_y', 'htop_mpi_inf_3dhp_z']  # List all 30 keypoints
output_columns = ['pelv_smpl_x', 'pelv_smpl_y', 'pelv_smpl_z', 'lhip_smpl_x', 'lhip_smpl_y', 'lhip_smpl_z', 'rhip_smpl_x', 'rhip_smpl_y', 'rhip_smpl_z', 'bell_smpl_x', 'bell_smpl_y', 'bell_smpl_z', 'lkne_smpl_x', 'lkne_smpl_y', 'lkne_smpl_z', 'rkne_smpl_x', 'rkne_smpl_y', 'rkne_smpl_z', 'spin_smpl_x', 'spin_smpl_y', 'spin_smpl_z', 'lank_smpl_x', 'lank_smpl_y', 'lank_smpl_z', 'rank_smpl_x', 'rank_smpl_y', 'rank_smpl_z', 'thor_smpl_x', 'thor_smpl_y', 'thor_smpl_z', 'ltoe_smpl_x', 'ltoe_smpl_y', 'ltoe_smpl_z', 'rtoe_smpl_x', 'rtoe_smpl_y', 'rtoe_smpl_z', 'neck_smpl_x', 'neck_smpl_y', 'neck_smpl_z', 'lcla_smpl_x', 'lcla_smpl_y', 'lcla_smpl_z', 'rcla_smpl_x', 'rcla_smpl_y', 'rcla_smpl_z', 'head_smpl_x', 'head_smpl_y', 'head_smpl_z', 'lsho_smpl_x', 'lsho_smpl_y', 'lsho_smpl_z', 'rsho_smpl_x', 'rsho_smpl_y', 'rsho_smpl_z', 'lelb_smpl_x', 'lelb_smpl_y', 'lelb_smpl_z', 'relb_smpl_x', 'relb_smpl_y', 'relb_smpl_z', 'lwri_smpl_x', 'lwri_smpl_y', 'lwri_smpl_z', 'rwri_smpl_x', 'rwri_smpl_y', 'rwri_smpl_z', 'lhan_smpl_x', 'lhan_smpl_y', 'lhan_smpl_z', 'rhan_smpl_x', 'rhan_smpl_y', 'rhan_smpl_z', 'nose_coco_x', 'nose_coco_y', 'nose_coco_z', 'leye_coco_x', 'leye_coco_y', 'leye_coco_z', 'lear_coco_x', 'lear_coco_y', 'lear_coco_z', 'reye_coco_x', 'reye_coco_y', 'reye_coco_z', 'rear_coco_x', 'rear_coco_y', 'rear_coco_z', 'htop_mpi_inf_3dhp_x', 'htop_mpi_inf_3dhp_y', 'htop_mpi_inf_3dhp_z']

# Create sequences of frames
sequence_length = 10  # Adjust as needed
X, y = [], []

for i in range(len(data) - sequence_length):
    X.append(data[input_columns].iloc[i:i+sequence_length].values)
    y.append(data[output_columns].iloc[i+sequence_length].values)

X = np.array(X)
y = np.array(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# Build the RNN model
model = Sequential()
model.add(LSTM(units=64, input_shape=(sequence_length, X_train.shape[-1])))
model.add(Dense(units=y_train.shape[1]))  # Output layer with the same number of features as y

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.1)

# Evaluate the model on the test set
loss = model.evaluate(X_test_scaled, y_test)
print(f'Test Loss: {loss}')

# Make predictions on new sequences
new_sequence = X_test_scaled[:4]  # Adjust as needed
predicted_frame = model.predict(new_sequence)

# Inverse transform the predicted frame to get the original scale
predicted_frame = scaler.inverse_transform(predicted_frame.reshape(-1, predicted_frame.shape[-1])).reshape(predicted_frame.shape)

# Print or use the predicted_frame as needed
print(predicted_frame)

# Make predictions on new sequences
new_sequence = X_test_scaled[:1]  # Adjust as needed
predicted_frame = model.predict(new_sequence)

# Inverse transform the predicted frame to get the original scale
predicted_frame = scaler.inverse_transform(predicted_frame.reshape(-1, predicted_frame.shape[-1])).reshape(predicted_frame.shape)

# Print or use the predicted_frame as needed
print(predicted_frame)
print(predicted_frame.shape)

######single new frame

fig = plt.figure(figsize=(10, 5.2))
pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
pose_ax.view_init(5, -75)
pose_ax.set_xlim3d(-1000, 1500)
pose_ax.set_zlim3d(1500, 4000)
pose_ax.set_ylim3d(1000, 6000)

pose_ax.set_xlim3d(-1000, 1500)
pose_ax.set_zlim3d(-500, 4000)
pose_ax.set_ylim3d(-1000, 6000)

poses3d = pred['poses3d'].numpy()
new_array = predicted_frame.reshape((30, 3))
new_array = new_array*1000
print(poses3d.shape)
new_array[..., 1], new_array[..., 2] = new_array[..., 2], new_array[..., 1]


# Add axis lines
pose_ax.quiver(0, 0, 0, 2000, 0, 0, color='r', label='X-axis')
pose_ax.quiver(0, 0, 0, 0, 2000, 0, color='g', label='Y-axis')
pose_ax.quiver(0, 0, 0, 0, 0, 2000, color='b', label='Z-axis')

pose_ax.legend()
for i_start, i_end in joint_edges:
    pose_ax.plot(*zip(new_array[i_start], new_array[i_end]), marker='o', markersize=2)

plt.savefig('3d_plot.png', format='png')
plt.show()

plt.savefig('3d_plot_rnn.png', format='png')

# Example: Create multiple new_arrays

# Your existing code for plotting


# Plot each new_array separately
    # Plot the points
# Plot each new_array separately
for array in predicted_frame:
  fig = plt.figure(figsize=(10, 5.2))
  pose_ax = fig.add_subplot(1, 2, 2, projection='3d')
  pose_ax.view_init(5, -75)  # Set initial viewing angles
  pose_ax.set_xlim3d(-1000, 1500)
  pose_ax.set_zlim3d(1500, 4000)
  pose_ax.set_ylim3d(1000, 6000)
    # Flip y-coordinates around the x-axis
  new_array = new_array.reshape((30, 3))
  new_array = new_array*1000
  new_array[..., 1] = -new_array[..., 1]
  print(new_array)

  # Plot the points
  for i_start, i_end in joint_edges:
      pose_ax.plot(*zip(new_array[i_start], new_array[i_end]), marker='o', markersize=2, label=f'Array {i + 1}')

  # Save the plot as a PNG file
  #plt.savefig(f'3d_plot_array_{i + 1}.png', format='png')

  # Clear the plot for the next iteration
  #pose_ax.cla()
  plt.show()

# Show the last plot (optional)
#plt.show()