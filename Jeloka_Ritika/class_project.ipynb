{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGYGbFaqUkHt"
      },
      "outputs": [],
      "source": [
        "## Load the MiniEcoset HDF5 file into a nested dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c_bIpAgZcqd"
      },
      "outputs": [],
      "source": [
        "!pip install h5py torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd0mA6wTdz4n"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGh2633TgFHl"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FULL DATASET\n",
        "def h5_to_dict(h5file):\n",
        "    \"\"\"Convert HDF5 group to nested dictionary.\"\"\"\n",
        "    result = {}\n",
        "    for key, item in h5file.items():\n",
        "        if isinstance(item, h5py.Dataset):  # Check if it's a dataset\n",
        "            result[key] = item[()]\n",
        "        elif isinstance(item, h5py.Group):  # Check if it's a group\n",
        "            result[key] = h5_to_dict(item)\n",
        "    return result\n",
        "\n",
        "filename = '/content/drive/MyDrive/miniecoset_64px.h5'\n",
        "with h5py.File(filename, 'r') as f:\n",
        "    data_dict = h5_to_dict(f)\n",
        "\n",
        "#print(data_dict)"
      ],
      "metadata": {
        "id": "XUgzeXrco65n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dict['val']['data'].shape)\n",
        "print(data_dict['categories'])\n",
        "print(data_dict['val']['labels'])"
      ],
      "metadata": {
        "id": "UnBSo2asKsdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "#PRINT IMAGES\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "image_data = data_dict['val']['data']\n",
        "\n",
        "# Assuming 'image_data' is a numpy array with shape (num_images, 64, 64, 3)\n",
        "num_images = image_data.shape[0]\n",
        "\n",
        "# Define the number of rows and columns for the grid\n",
        "num_rows = 20  # Adjust the number of rows as needed\n",
        "num_cols = 5  # Adjust the number of columns as needed\n",
        "\n",
        "# Create a grid of subplots to display the images\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(40, 40))\n",
        "\n",
        "# Iterate through the images and display them in subplots\n",
        "for i in range(num_rows):\n",
        "    for j in range(num_cols):\n",
        "        index = i * num_cols + j\n",
        "        if index < num_images:\n",
        "            image = image_data[index]\n",
        "            axes[i, j].imshow(image)\n",
        "            axes[i, j].axis('off')  # Hide axis labels and ticks\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "\n",
        "# Show the grid of images\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kq2QhAfuKjRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dict['id_of_categories_in_ecoset'])\n",
        "print(data_dict['id_of_categories_in_ecoset'].shape)"
      ],
      "metadata": {
        "id": "8oIqYgo0nvcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dict['categories'])\n",
        "print(data_dict['categories'].shape)"
      ],
      "metadata": {
        "id": "yIrl5MvqzIrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_dict)"
      ],
      "metadata": {
        "id": "Z0-ZQscr0r8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcRsZbGmkF9J"
      },
      "source": [
        "### Finetune ResNet50 on MiniEcoset using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqfsz2lpkLJo"
      },
      "outputs": [],
      "source": [
        "##**Step 1: Prepare the Data Loaders*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ijeG7mv8Qbg"
      },
      "outputs": [],
      "source": [
        "class MiniEcosetDataset(Dataset):\n",
        "    def __init__(self, hdf5_file, subset, transform=None):\n",
        "        self.hdf5_file = hdf5_file\n",
        "        self.subset = subset  # this should be 'train', 'val', 'test', etc.\n",
        "        self.transform = transform\n",
        "\n",
        "        with h5py.File(hdf5_file, 'r') as file:\n",
        "          self.length = len(file[f'{self.subset}/data'])  # access the data using the subset\n",
        "\n",
        "        # Load labels from the \"train\" group\n",
        "          self.labels = file[f'{self.subset}/labels']\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with h5py.File(self.hdf5_file, 'r') as file:\n",
        "            # Access the data and labels using the subset\n",
        "            image = file[f'{self.subset}/data'][idx]\n",
        "            label = file[f'{self.subset}/labels'][idx]\n",
        "\n",
        "        # Convert image from numpy array to torch tensor\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1).float()  # CxHxW format\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbFOxoLH6874"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalizing according to ResNet50 requirements\n",
        "])\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = MiniEcosetDataset(hdf5_file='/content/drive/MyDrive/miniecoset_64px.h5', subset='train', transform=transform)\n",
        "val_dataset = MiniEcosetDataset(hdf5_file='/content/drive/MyDrive/miniecoset_64px.h5', subset='val', transform=transform)\n",
        "# similarly, you can create for 'test' or 'testplus' subsets\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58upTauPkLOe"
      },
      "outputs": [],
      "source": [
        "##*Step 2: Load a Pretrained ResNet50 and Modify the Last Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUpfrt0b8p8P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load a pre-trained ResNet50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# If you want to freeze the weights of the pre-existing layers, you can do so like this:\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Now, you should replace the last layer with a new layer that has the same input dimension,\n",
        "# but the output dimension should be the number of classes in your dataset.\n",
        "# You mentioned a \"mini ecosystem,\" but didn't specify the number of classes. Let's assume it's `num_classes`.\n",
        "\n",
        "num_classes = 100  # replace with your number of classes\n",
        "\n",
        "# ResNet models have their fully connected layer at 'fc',\n",
        "# we'll replace it with a new one with the appropriate number of outputs\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Now the model is ready, and you can train it on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGL6cMpwkLWo"
      },
      "outputs": [],
      "source": [
        "##*Step 3: Train the Model if Fine-tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TORKkARB9MQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70332e9b-fc4a-4d55-f9f7-2fecd3ccae7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.6796\n",
            "Validation Loss: 2.5135, Accuracy: 44.72%\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming your dataset is ready and is named 'train_dataset' and 'val_dataset'\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define a loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can change the learning rate\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 10  # You can change the number of epochs\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        labels = labels.long()  # Convert labels to LongTensor\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute the loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # In the validation phase, we don't need to compute gradients\n",
        "        for images, labels in val_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            labels = labels.long()  # Convert labels to LongTensor\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "    val_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Validation Loss: {val_epoch_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl7BH9IZkDWI"
      },
      "source": [
        "### Visualize the activations from the ResNet50 model's feature layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFRv8DSZjvFt"
      },
      "outputs": [],
      "source": [
        "##*1. Modify ResNet50 to Return the 2048 Neuron Activations:**\n",
        "##To modify ResNet50 to return the activations, we remove the last fully connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pql-AU_HrWEb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class ResNet50FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50FeatureExtractor, self).__init__()\n",
        "        # Load the pretrained ResNet-50 model\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        # Remove the last fully connected layer (layer4)\n",
        "        self.features = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
        "        # Optionally, remove the global average pooling layer\n",
        "        # self.features = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass, get the feature vector before the final fully connected layer\n",
        "        x = self.features(x)\n",
        "        # If you're not removing the AvgPool, you'll end up with a 4D tensor\n",
        "        # You might want to modify the shape to be more manageable\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "# Now create the model\n",
        "model = ResNet50FeatureExtractor()\n",
        "\n",
        "# Example of passing a dummy input through the model\n",
        "# Remember, input size for ResNet50 should be at least 224x224\n",
        "with torch.no_grad():  # No need to track the gradients\n",
        "    dummy_input = torch.randn(1, 3, 224, 224)  # Dummy input\n",
        "    features = model(dummy_input)  # Extract features\n",
        "    print(features.shape)  # Should show the feature vector shape, e.g., [1, 2048]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYdbUOWMjvMD"
      },
      "outputs": [],
      "source": [
        "##**2. Extract and Save Activations:*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#activations = np.concatenate(activations, axis=0)"
      ],
      "metadata": {
        "id": "aPtw8DD_yFCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)\n",
        "_ = next(iter(train_loader))\n",
        "len(_)\n",
        "_[0].shape, _[1].shape"
      ],
      "metadata": {
        "id": "oWXMx4-o4PYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkWvffxtrhV3"
      },
      "outputs": [],
      "source": [
        "#ALWAYS TIMES OUT AFTER THIS STEP\n",
        "import torch\n",
        "\n",
        "# Assuming train_loader is your DataLoader containing the dataset\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "activations = []\n",
        "\n",
        "with torch.no_grad():  # No need to track the gradients\n",
        "    for batch in val_loader: # NOTE: you want to use test or testplus instead here\n",
        "        images, _ = batch  # Get the images from the batch; ignore labels\n",
        "        features = model(images)  # Pass the batch through the model to get the activations\n",
        "        activations.append(features.cpu().numpy())  # Save the activations\n",
        "\n",
        "# Concatenate all the activations retrieved\n",
        "activations = np.concatenate(activations, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activations.shape"
      ],
      "metadata": {
        "id": "M5Rle5g05EKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1CuwmJ9riBp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Save with numpy\n",
        "np.save('activations.npy', activations)\n",
        "\n",
        "# Or save with torch\n",
        "#torch.save(activations, 'activations.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvxSj5sFVPvU"
      },
      "outputs": [],
      "source": [
        "##**3. Apply PCA to Reduce Dimensionality:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHGfUhFCrq2R"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8Xv5ENarrXw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load activations\n",
        "# Choose the correct file loading based on how you saved your activations\n",
        "activations = np.load('activations.npy')  # If saved with numpy\n",
        "# activations = torch.load('activations.pt')  # If saved with torch\n",
        "# with h5py.File('activations.h5', 'r') as f:  # If saved in HDF5 format\n",
        "#     activations = f['activations'][:]\n",
        "\n",
        "# Initialize PCA; you can specify the number of components as a parameter\n",
        "# For example, PCA(n_components=50) for the 50 principal components.\n",
        "# If not specified, all components are kept.\n",
        "pca = PCA()\n",
        "\n",
        "# Fit PCA on your data and transform it\n",
        "principal_components = pca.fit_transform(activations)\n",
        "#component_labels = pca.fit_predict(activations)\n",
        "\n",
        "# Now, principal_components are your activations in the reduced-dimensional space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuckGR78rvSS"
      },
      "outputs": [],
      "source": [
        "# Save the reduced data\n",
        "np.save('reduced_activations.npy', principal_components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJtunwdEjpjz"
      },
      "outputs": [],
      "source": [
        "##**4. Visualize the 2D Representations:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoRM9u9vryIR"
      },
      "outputs": [],
      "source": [
        "#ELBOW PLOT\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "plt.figure()\n",
        "plt.plot(cumulative_explained_variance)\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Component-wise and Cumulative Explained Variance')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2D PCA\n",
        "component_labels = data_dict['val']['labels']\n",
        "# Visualize the data in the PCA space\n",
        "colormap = plt.cm.get_cmap('viridis', len(np.unique(component_labels)))\n",
        "\n",
        "# Scatter plot with colors based on components\n",
        "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=component_labels, cmap=colormap)\n",
        "\n",
        "#plt.scatter(principal_components[:, 0], principal_components[:, 1])\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('PCA Visualization')\n",
        "plt.legend()\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kXX5fexZiMmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3D PCA\n",
        "# Compute component labels (you can replace this with your actual labels)\n",
        "component_labels = data_dict['val']['labels']\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Scatter plot with colors based on components\n",
        "scatter = ax.scatter(principal_components[:, 0], principal_components[:, 1], principal_components[:, 2], c=component_labels, cmap='viridis')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "plt.title('PCA in 3D with Colors')\n",
        "\n",
        "# Create a colorbar to indicate the mapping of labels to colors\n",
        "cbar = plt.colorbar(scatter, label='Component Labels')\n",
        "\n",
        "# Show"
      ],
      "metadata": {
        "id": "V3sAYHX9DA1E"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}